{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chapter 2 Linear Algebra\n",
    "\n",
    "### 2.1 Scalars, Vectors, Matrices and Tensors\n",
    "\n",
    "### 2.2 Multiplying Matrices and Vectors\n",
    "\n",
    "- element-wise product Hadamard product\n",
    "\n",
    "### 2.3 Identity and Inverse Matrices\n",
    "\n",
    "- $x = A^{-1}b$ is primarily useful as a theoretical tool, because $A^{-1}$ can be represented with only precision on a digital computer, algorithms that makes use of the value of $b$ can usually obtain more accurate estimates of $x$.\n",
    "\n",
    "### 2.4 Linear Dependence and Span\n",
    "\n",
    "- We can think of the columns of $A$ as specifying different directions we can travel from the **origin**, and determine how many ways there are of reaching $b$.\n",
    "\n",
    "$$Ax = \\sum_i x_i A_{:,i}$$\n",
    "\n",
    "- Then **span** of a set of vectors is the set of all points obtainable by linear combination of original vectors.\n",
    "\n",
    "- A set of vectors is **linearly independent** if no vector in the set is a linear combination of the other vectors.\n",
    "\n",
    "- No set of m-dimensional vectors can have more than m mutually liearnly independent columns, but a matrix with more than m columns may have more than one such set.\n",
    "\n",
    "- A square matrix with linearly dependent columns is known as **singular**\n",
    "\n",
    "### 2.5 Norms\n",
    "\n",
    "- a function used to measure the size of a **vector**\n",
    "\n",
    "$$ ||x||_p = (\\sum_i |x_i|^p)^{\\frac{1}{p}} \\, \\forall p \\in \\mathbb{R}, p \\geq 1.$$\n",
    "\n",
    "- squared $L^2$ norm may be undesirable in many contexts, because it increases very slowly near the origin. In several ML applications, it is important to discriminate between elements that are exactly zero and elements that are small but nonzero.\n",
    "\n",
    "- The $L^1$ norm is commonly used in ML when the difference between zero and nonzeros elements is very important.\n",
    "\n",
    "- **incorrect terminology** $L^0 norm$, the number of nonzeros in vector, because scaling the vector by $\\alpha$ does not change the number of nonzero entries.\n",
    "\n",
    "- **max norm**($L^{\\infty}$): $||x||_{\\infty} = \\max_i |x_i|$\n",
    "\n",
    "- **Frobenius norm**: measure the size of a matrix, which is analogous to the $L^2$ norm of a vector.\n",
    "\n",
    "- $x^T y = ||x||_2 ||y||2 \\cos \\theta$, where $\\theta$ is the angle between $x$ and $y$. But I don't known the meaning of this equation.\n",
    "\n",
    "### 2.6 Special Kinds of Matrices and Vectors\n",
    "\n",
    "- **Diagonal**: $diag(v)$ denote a square diagonal matrix whose diagonal entries are given by the entries of the vector $v$.\n",
    "\n",
    "- $diag(v)x = v \\odot x$\n",
    "\n",
    "- Ihe inverse of a square diagonal exists only if every diagonal entry is nonzero\n",
    "\n",
    "$$diag(v)^{-1} = diag([1/v_1,\\dots,1/v_n]^T)$$\n",
    "\n",
    "- Non-square diagonal matrix $D$. $Dx$ will involve scaling element of $x$, and either concatenating some zeros to the result if $D$ is taller than it is wide, or discarding some of the last elements of the vector if $D$ is wider than it is tall.\n",
    "\n",
    "- Symmetric matrix\n",
    "\n",
    "- **Unit vector** is a vector with **unit norm**\n",
    "\n",
    "- A vector $x$ and a vector $y$ are **orthogonal(正交)** to each other if $x^Ty = 0$. If both vectors have nonzero norm, this means that they are at a 90 degree angle to each other.\n",
    "\n",
    "- **orthonormal(正交单位向量)**: vectors are not only orthogonal but alse have unit norm.\n",
    "\n",
    "- **orthogonal matrix** is a square matrix whose rows are mutually **orthonormal** and whose columns are mutually **orthonormal**.\n",
    "\n",
    "$$A^TA = AA^T = I$$\n",
    "$$A^{-1} = A^T$$\n",
    "\n",
    "### 2.7 Eigendecomposition\n",
    "\n",
    "- (Why) we decompose matrices in ways that shows us infomation about their functional properties that is not obvious from the representation of the matrix as an array of elements.\n",
    "\n",
    "- An **eigenvector** of a square matrix $A$ is a non-zero vector $v$ such that multiplication by $A$ alters only the scale of $v$, the scalar $\\lambda$ is known as the **eigenvalue** corresponding to this eigenvector.\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "- Suppose that a matrix $A$ has $n$ linearly indendent eigenvectors, $v^{(1)},\\dots,v^{(n)}$, with corresponding eigenvalues ${\\lambda_1,\\dots,\\lambda_n}$. We may concatenate all of the eigenvectors to form a matrix $V$ with one eigenvector per column: $V = [v_{(1)},\\dots,v_{(n)}]$. Likewise, we can concatenate the eigenvalues to form a vector $\\lambda = [\\lambda_1,\\dots,\\lambda]^T$. The **eigendecomposition** of $A$ is then given by\n",
    "\n",
    "$$\\boldsymbol{A} = \\boldsymbol{V} diag(\\boldsymbol{\\lambda}) \\boldsymbol{V}^{-1}$$\n",
    "\n",
    "- Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:\n",
    "\n",
    "$$\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^T$$\n",
    "\n",
    "- where $\\boldsymbol{Q}$ is an orthogonal matrix of A, and $\\boldsymbol{\\Lambda}$ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "- The matrix is singular if and only if any of the eigenvalues are zero.\n",
    "\n",
    "- $f(x) = x^TAx subject to ||x||_2 = 1$. Whenever $x$ is equal to an eigenvector of $A$, f takes the value of the corresponding eigenvalue.\n",
    "\n",
    "- **positive definite**: A matrix whose eigenvalues are all positive.\n",
    "- **positive semidefinite**: eigenvalues are all positive or zero-valued.\n",
    "- **negative difinite**: eigenvalues are all negative.\n",
    "- **negative semidefinite**: eigenvalues are all negative or zero-valued.\n",
    "\n",
    "- Positive semidefinite: $\\forall x, x^TAx \\geq 0.$\n",
    "- Positive definite: $x^TAx = 0 \\implies x = 0$\n",
    "\n",
    "### 2.8 Singular Value Decomposition\n",
    "\n",
    "- Every real matrix has a sigular value decomposition, but the same is not true of the eigenvalue decomposition.\n",
    "\n",
    "- **SVD**:$A = UDV^T$, where $A$ $m \\times n$, $U$ $m \\times m$, $D$ $m \\times n$, $V$ $n \\times n$\n",
    "\n",
    "- $U$ and $V$ are both defined to be orthogonal matrices. $D$ is defined to be a diagonal matrix(not necessarily square.\n",
    "\n",
    "- **singular values** of $A$: the elements along the diagonal of $D$\n",
    "- **left-singular vectors**: $U$\n",
    "- **right-singular vectors**: $V$\n",
    "\n",
    "- **Perhaps mose useful feature of the SVD is that we can use it to partially generalize matrix inversion to non-square matrices**.\n",
    "\n",
    "### 2.9 The Moore-Penrose Pseudoinverse(广义逆)\n",
    "\n",
    "- solve a linear equation $Ax = y$, if $A$ is taller than it is wide, then it is possible to have no solution. If $A$ is wider than it is tall, then there could be multiple possible solutions.\n",
    "\n",
    "$$ A^+ = VD^+U^T $$\n",
    "\n",
    "- where $U$, $D$ and $V$ are SVD decomposition of $A$, $D^+$ is obtained by taking the reciprocal of non-zero elements of $D$ then taking the transpose of the resulting matrix.\n",
    "\n",
    "- When $A$ has more columns than rows, $x = A^+y$ provide the solution with minimal Euclidean norm $||x||_2$\n",
    "\n",
    "- When $A$ has more rows than columns, using the pseudoinverse gives the $x$ for which $Ax$ is as close as possible to $y$ in terms of Euclidean norm $||Ax-y||_2$\n",
    "\n",
    "### 2.10 The Trace Operator\n",
    "\n",
    "- $Tr(A) = \\sum_i A_{i,i}$\n",
    "- $Tr(A) = Tr(A^T)$\n",
    "- $Tr(ABC) = Tr(CAB) = Tr(BCA)$\n",
    "- $Tr(AB) = Tr(BA)$ even though $AB \\in \\mathbb{R}^{m \\times m}$ and $BA \\in \\mathbb{R}^{n \\times n}$\n",
    "- scalar: $Tr(a) = a$\n",
    "\n",
    "### 2.11 The Determinant\n",
    "\n",
    "- $det(A)$ of a square matrix: product of all the eigenvalues of the matrix.\n",
    "- $abs(det(A))$ can be thought of as a measure of how much multiplication by the matrix expands or contracts space. If det is 0, then space is contracted completely along at least one dimension. If det is 1, then the transformation preserves volume.\n",
    "\n",
    "### 2.12 Example: Principal Components Analysis\n",
    "\n",
    "- given a unit vector $u$ and a point $x$, the length of the projection of $x$ onto $u$ is given by $x^Tu$. ie. if $x^{(i)}$is a point in dataset then its projection onto $u$ is distance $x^Tu$ from the origin. Hence, **to maximize the variance of the projctions**, we would like to choose a unit-length $u$ so as to maximize:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{1}{m} \\sum_{i=1}^m ({x^{(i)}}^T u)^2\n",
    "  = & \\frac{1}{m} \\sum_{i=1}^m u^T x^{(i)} {x^{(i)}}^T u \\\\\n",
    "  = & u^T \\bigg( \\frac{1}{m} \\sum_{i=1}^m x^{(i)} {x^{(i)}}^T \\bigg) u\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- to maximizing this subject to $^||u||_2 = 1$ gives the pricipal eigenvector of $\\Sigma$, which is just the empirical covariance matrix of the data(zero mean).\n",
    "\n",
    "- generally, if we wish to project data into a k-dimensional subspace (k < n), we should choose $u_1,u_2,\\dots,u_k$ to be the top k eigenvectors of $\\Sigma$.\n",
    "\n",
    "- PCA can also be derived by picking the basis that **minimizes the approximation error arising from projecting the data onto the k-dimensional subspace spanned by them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
