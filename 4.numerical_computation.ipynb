{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Numerical Computation\n",
    "\n",
    "## 4.1 Overflow and Underflow\n",
    "\n",
    "- evaluate $softmax(z)$ where $z = x - max_i x_i$ instead of $softmax(x)$. substracting $max_i x_i$ results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out the possibility of underflow in the denominator leading to a division by zero.\n",
    "\n",
    "$$softmax(x)_i = \\frac{exp(x_i)} {\\sum_{j=1}^{n} exp(x_j)}$$\n",
    "\n",
    "## 4.2 Poor Conditioning\n",
    "\n",
    "- **condition number**: $f(x) = A^{-1}x$. When $A \\in \\mathbb{R}^{n \\times n}$ has a eigenvalue decomposition, its **condition number** is\n",
    "\n",
    "$$ \\displaystyle \\max_{i,j} \\bigg|\\frac {\\lambda_i} {\\lambda_j}\\bigg| $$\n",
    "\n",
    "- When this number is large, matrix inversion is particularly sensitive to error in the input.\n",
    "\n",
    "- This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion.\n",
    "\n",
    "## 4.3 Gradient-Based Optimization\n",
    "\n",
    "- $f(x+\\epsilon) \\approx f(x) + \\epsilon f^{\\prime}(x)$\n",
    "- **stationary points** or **critical points**: $f^{\\prime}(x) = 0$\n",
    "- **saddle points**\n",
    "- The general concept of gradient descent can be generalized to discrete spaces. **hill climbing**\n",
    "\n",
    "### 4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices\n",
    "\n",
    "- **Jacobian matrix**: $J \\in \\mathbb{R}^{n \\times m}$ of $f$ is defined such that $J_{i,j}=\\frac{\\partial}{\\partial x_j} f(x)_i$\n",
    "\n",
    "- We can think of the **second derivative** as measuring **curvature**.\n",
    "  - zero: flat line\n",
    "  - positive: the function curves upward\n",
    "  - negative: the function curves downward\n",
    "\n",
    "- **Hessian matrix**:\n",
    "\n",
    "$$H(f)(x)_{i,j} = \\frac {\\partial^2} {\\partial x_i \\partial x_j} f(x)$$\n",
    "\n",
    "- the Hessian is the Jacobian of the gradient.\n",
    "\n",
    "- symmetric:\n",
    "\n",
    "$$\n",
    "\\frac {\\partial^2} {\\partial x_i \\partial x_j} f(x) =\n",
    "\\frac {\\partial^2} {\\partial x_j \\partial x_i} f(x)\n",
    "$$\n",
    "\n",
    "- We can make a second-order Tyalor series approximation to the function $f(x)$ around the current point $x^{(0)}$:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(x^{(0)}) + (x - x^{(0)})^T g\n",
    "+ \\frac {1}{2} (x - x^{(0)})^T H(x-x^{(0)})\n",
    "$$\n",
    "\n",
    "- learning rate $\\epsilon$, new point $x$ will given by $x^{(0)}-\\epsilon g$:\n",
    "\n",
    "$$\n",
    "f(x^{(0)}-\\epsilon g) \\approx f(x^{(0)})-\\epsilon g^Tg+\\frac{1}{2}\\epsilon^2g^THg\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^\\ast = x^{(0)} - H(f)(x^{(0)})^{-1}\\nabla_xf(x^{(0)})\n",
    "$$\n",
    "\n",
    "- **first-order optimization algorithms** use only the gradient, and **second-order optimization algorithms** use the Hessian matrix, such as **Newton's method**.\n",
    "\n",
    "## 4.4 Constrained Optimization\n",
    "\n",
    "- One simple approach to constrained optimizationis imply to modify gradient descent taking the constraint into account.\n",
    "\n",
    "- A more sophisticated approach is to design a unconstrained optimization problem whose solution can be converted into solution to the original.\n",
    "\n",
    "$$g(\\theta) = f([\\cos\\theta,\\sin\\theta]^T)$$\n",
    "\n",
    "- then return $[\\cos\\theta, \\sin\\theta]$ as the solution to the original problem.\n",
    "\n",
    "- **KKT**: general solution to constrained optimization. **generalized Lagrange function**\n",
    "\n",
    "## 4.5 Example: Linear Least Squares\n",
    "\n",
    "\n",
    "\n",
    "### reference\n",
    "\n",
    "- Intuitive overview of taylor series: http://davidlowryduda.com/an-intuitive-overview-of-taylor-series/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
