{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Deep Feedforward Networks\n",
    "\n",
    "## 6.1 Example: Learning XOR\n",
    "\n",
    "## 6.2 Gradient-Based Learning\n",
    "\n",
    "### 6.2.1 Cost Function\n",
    "\n",
    "- We can view the cost function as being a **functional**(泛函) rather than just a function. We can thus think of learning as choosing a function rather than merely choosing a set of parameters.\n",
    "\n",
    "- The **calculus of variantions**(变分法) can be used to derive:\n",
    "  - If we could train on ifinitely many samples from true data generating distribution, minimizing MSE gives a function that predicts the mean of $y$ for each value of $x$.\n",
    "  - **Mean absolute error** cost function will predicts the *median* value of the $y$ for each value of $x$.\n",
    "\n",
    "- MSE and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cose functions. This is one reason that the cross-entropy cost function is more popular than MSE.\n",
    "\n",
    "### 6.2.2 Output Units\n",
    "\n",
    "- The choice of cost function is tightly coupled with choice of output unit.\n",
    "\n",
    "#### 6.2.2.1 Linear Units for Gaussian Output Distributions\n",
    "\n",
    "- Maximizing the log-likelihood is equivalent to minimizing the MSE.\n",
    "- The maximum likelihood framework makes it straightforward to learn the covariance of the Gaussian too.\n",
    "\n",
    "#### 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions\n",
    "\n",
    "- If we begin with assumption that the unnormalized log probabilities $\\tiled{P}(y)$ are linear in $y$ and $z$, we can exponentiate to obtain the unnormalized probabilities and then normalize to see that yields a Bernoulli distribution controlled by a sigmoidal transformation of $z$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log \\tilde{P}(y) & = yz \\\\\n",
    "\\tilde{P}(y) & = exp(yz) \\\\\n",
    "P(y) & = \\frac {exp(yz)} {\\sum_{y' = 0}^1 exp(y'z)} \\\\\n",
    "P(y) & = \\sigma ((2y-1)z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- loss function for maximum likelihood learning of a Bernoulli parametrized by a sigmoid is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) & = -\\log P(y|x)  \\\\\n",
    "          & = -\\log \\sigma((2y-1)z) \\\\\n",
    "          & = \\zeta((1-2y)z)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- The cost function used with maximum likelihood is $-\\log P(y|x)$, the $\\log$ in the cost function undoes the exp of the sigmoid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
